name: Test
on:
  pull_request:
    branches: [ master ]
  push:
    branches:
      - master

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout
      uses: actions/checkout@v2
   
    - name: Set up Python 3.x
      uses: actions/setup-python@v1
      with:
        python-version: '3.x'

    - name: Upgrade pip
      run: |
        python -m pip install --upgrade pip
    - name: Install GET
      run:
        sudo apt-get install libwww-perl      
    - name: Install libgdal-dev and gdal
      run: |
        sudo apt-get update
        sudo apt-get install libgdal-dev
        export CPLUS_INCLUDE_PATH=/usr/include/gdal
        export C_INCLUDE_PATH=/usr/include/gdal
        
    - name: Install fiona, wheel, fitz and PyMuPDF
      run: |
        pip install Fiona
        pip install wheel
        pip install fitz
        pip install PyMuPDF

    - name: Install python dependencies
      run: |
        pip install -r requirements.txt
        
    - name: Install R 
      run: |
        sudo apt install r-base
        
    - name: Install Chromium
      run: |
        sudo apt-get clean 
        sudo apt-get update
        sudo apt-get install chromium-browser

    - name: Install Tree
      run: |
        sudo apt-get install tree

    - name: Clone data from yesterday
      run: |
        git clone https://${GITHUB_ACTOR}:${{ secrets.GITHUB_TOKEN }}@github.com/MJHutchinson/US-covid19-data-scraping --single-branch -b update_data output 
        git config --global user.name "GitHub Actions"
        git config --global user.email action@github.com
    - name: Cache, process and update
      run: |
        make files
        today=$(date +'%Y-%m-%d')
        
        # check the data for today is there already
        cp -a data/. output/data/
        cp -a html/. output/html/
        cp -a csvs/. output/csvs/
        cp -a pngs/. output/pngs/

        cd output

        tree csvs -L 2
        tree html -L 2
        tree pngs -L 2
    - name: Test AWSCLI output
     run: |
      # print out some filenames
      aws s3 ls s3://$AWS_BUCKET/update_data/pdfs/florida/